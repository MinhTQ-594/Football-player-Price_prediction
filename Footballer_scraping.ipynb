{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from functools import reduce\n",
    "import sys\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "'''\n",
    "This program will get summary player data for each game played in the top 5 \n",
    "European football leagues from the website fbref.com\n",
    "'''\n",
    "\n",
    "def get_data_info():\n",
    "    # all possible leagues and seasons\n",
    "    leagues = ['Premier League', 'La Liga', 'Serie A', 'Ligue 1', 'Bundesliga']\n",
    "    seasons = ['2017-2018', '2018-2019', '2019-2020', '2020-2021', '2021-2022', '2022-2023', '2023-2024']\n",
    "    \n",
    "    while True:\n",
    "        # select league [Premier League / La Liga / Serie A / Ligue 1 / Bundesliga]\n",
    "        league = input('Select League (Premier League / La Liga / Serie A / Ligue 1 / Bundesliga): ')\n",
    "        \n",
    "        # check if input valid\n",
    "        if league not in leagues:\n",
    "            print('League not valid, try again')\n",
    "            continue\n",
    "            \n",
    "        # assign url names and id's\n",
    "        if league == 'Premier League':\n",
    "            league = 'Premier-League'\n",
    "            league_id = '9'\n",
    "\n",
    "        if league == 'La Liga':\n",
    "            league = 'La-Liga'\n",
    "            league_id = '12'\n",
    "\n",
    "        if league == 'Serie A':\n",
    "            league = 'Serie-A'\n",
    "            league_id = '11'\n",
    "\n",
    "        if league == 'Ligue 1':\n",
    "            league = 'Ligue-1'\n",
    "            league_id = '13'\n",
    "\n",
    "        if league == 'Bundesliga':\n",
    "            league = 'Bundesliga'\n",
    "            league_id = '20'\n",
    "        break\n",
    "            \n",
    "    while True: \n",
    "        # select season after 2017 as XG only available from 2017,\n",
    "        season = input('Select Season (2017-2018 to 2023-2024): ')\n",
    "        \n",
    "        # check if input valid\n",
    "        if season not in seasons:\n",
    "            print('Season not valid, try again')\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    url = f'https://fbref.com/en/comps/{league_id}/{season}/schedule/{season}-{league}-Scores-and-Fixtures'\n",
    "    return url, league, season\n",
    "\n",
    "\n",
    "def get_fixture_data(url, league, season):\n",
    "    print('Getting fixture data...')\n",
    "    # create empty data frame and access all tables in url\n",
    "    fixturedata = pd.DataFrame([])\n",
    "    tables = pd.read_html(url)\n",
    "    \n",
    "    # get fixtures\n",
    "    fixtures = tables[0][['Wk', 'Day', 'Date', 'Time', 'Home', 'Away', 'xG', 'xG.1', 'Score']].dropna()\n",
    "    fixtures['season'] = url.split('/')[6]\n",
    "    fixturedata = pd.concat([fixturedata, fixtures])\n",
    "    \n",
    "    # assign id for each game\n",
    "    fixturedata[\"game_id\"] = fixturedata.reset_index().index - 2\n",
    "    \n",
    "    # export to csv file\n",
    "    fixturedata.reset_index(drop=True).to_csv(f'Data/RAW/FBREF_Dataset/{season.lower()}/{league.lower()}_{season.lower()}_fixture_data.csv', \n",
    "        header=True, index=False, mode='w')\n",
    "    print('Fixture data collected...')\n",
    "\n",
    "\n",
    "def get_match_links(url, league):   \n",
    "    print('Getting player data...')\n",
    "    # access and download content from url containing all fixture links    \n",
    "    match_links = []\n",
    "    html = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    links = soup(html.content, \"html.parser\").find_all('a')\n",
    "    \n",
    "    # filter list to return only needed links\n",
    "    key_words_good = ['/en/matches/', f'{league}']\n",
    "    for l in links:\n",
    "        href = l.get('href', '')\n",
    "        if all(x in href for x in key_words_good):\n",
    "            if 'https://fbref.com' + href not in match_links:                 \n",
    "                match_links.append('https://fbref.com' + href)\n",
    "    return match_links\n",
    "\n",
    "\n",
    "\n",
    "def player_data(match_links, league, season):\n",
    "    # loop through all fixtures\n",
    "    player_data = pd.DataFrame([])\n",
    "    for count, link in enumerate(match_links):\n",
    "        try:\n",
    "            html = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            tables = pd.read_html(html.content)\n",
    "            for table in tables:\n",
    "                try:\n",
    "                    table.columns = table.columns.droplevel()\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            def rename_columns(data_frames):\n",
    "                prefixes = ['Summary_', 'Passing_', 'Pass_Types_', 'Defensive_Actions_', 'Possession_', 'Miscellaneous_Stats_', 'Goalkeeping_']\n",
    "\n",
    "                for i, df in enumerate(data_frames):\n",
    "                    if i == 0:  # table[3]\n",
    "                        renamed = False\n",
    "                        new_columns = []\n",
    "\n",
    "                        for idx, col in enumerate(df.columns):\n",
    "                            if col == 'Att':\n",
    "                                if not renamed:\n",
    "                                    new_columns.append('pass_' + col)\n",
    "                                    renamed = True\n",
    "                                else:\n",
    "                                    new_columns.append('takes_on_' + col)  \n",
    "                            else:\n",
    "                                new_columns.append(col)  # Keep the original name for other columns\n",
    "\n",
    "                        df.columns = new_columns  # Assign the new column names at once\n",
    "                    elif i == 1:  # table[4]\n",
    "                        new_columns = []\n",
    "                        cmp_count = 0\n",
    "                        att_count = 0\n",
    "                        cmp_percent_count = 0\n",
    "\n",
    "                        for col in df.columns:\n",
    "                            if col == 'Cmp':\n",
    "                                if cmp_count == 0:\n",
    "                                    new_columns.append('total_' + col)\n",
    "                                elif cmp_count == 1:\n",
    "                                    new_columns.append('short_' + col)\n",
    "                                elif cmp_count == 2:\n",
    "                                    new_columns.append('medium_' + col)\n",
    "                                elif cmp_count == 3:\n",
    "                                    new_columns.append('long_' + col)\n",
    "                                cmp_count += 1 \n",
    "\n",
    "                            elif col == 'Att':\n",
    "                                if att_count == 0:\n",
    "                                    new_columns.append('total_' + col)\n",
    "                                elif att_count == 1:\n",
    "                                    new_columns.append('short_' + col)\n",
    "                                elif att_count == 2:\n",
    "                                    new_columns.append('medium_' + col)\n",
    "                                elif att_count == 3:\n",
    "                                    new_columns.append('long_' + col)\n",
    "                                att_count += 1\n",
    "\n",
    "                            elif col == 'Cmp%':\n",
    "                                if cmp_percent_count == 0:\n",
    "                                    new_columns.append('total_' + col)\n",
    "                                elif cmp_percent_count == 1:\n",
    "                                    new_columns.append('short_' + col)\n",
    "                                elif cmp_percent_count == 2:\n",
    "                                    new_columns.append('medium_' + col)\n",
    "                                elif cmp_percent_count == 3:\n",
    "                                    new_columns.append('long_' + col)\n",
    "                                cmp_percent_count += 1\n",
    "\n",
    "                            else:\n",
    "                                new_columns.append(col)  # Keep original name\n",
    "\n",
    "                        df.columns = new_columns  # Assign the new names at once            \n",
    "                                \n",
    "                    elif i == 3:  # table[6]\n",
    "                        new_columns = []\n",
    "                        tkl_count = 0\n",
    "\n",
    "                        for col in df.columns:\n",
    "                            if col == 'Tkl':\n",
    "                                if tkl_count == 0:\n",
    "                                    new_columns.append('total_' + col)\n",
    "                                elif tkl_count == 1:\n",
    "                                    new_columns.append('dribblers_' + col)\n",
    "                                tkl_count += 1\n",
    "                            else:\n",
    "                                new_columns.append(col)\n",
    "\n",
    "                        df.columns = new_columns  # Assign the new names at once\n",
    "\n",
    "                    df.columns = [prefixes[i] + col if col not in ['Player', 'Nation', 'Age', 'Min'] else col for col in df.columns]\n",
    "\n",
    "            # get player data\n",
    "            def get_team_1_player_data():\n",
    "                # outfield and goal keeper data stored in separate tables \n",
    "                data_frames = [tables[i] for i in range(3, 4)]\n",
    "                rename_columns(data_frames)\n",
    "                # merge outfield and goal keeper data\n",
    "                df = reduce(lambda left, right: pd.merge(left, right, \n",
    "                    on=['Player', 'Nation', 'Age', 'Min'], how='outer'), data_frames).iloc[:-1]\n",
    "                \n",
    "                # assign a home or away value\n",
    "                return df.assign(home=1, game_id=count)\n",
    "\n",
    "            # get second team's player data        \n",
    "            def get_team_2_player_data():\n",
    "                data_frames = [tables[i] for i in range(10, 11)]\n",
    "                rename_columns(data_frames)\n",
    "                df = reduce(lambda left, right: pd.merge(left, right,\n",
    "                    on=['Player', 'Nation', 'Age', 'Min'], how='outer'), data_frames).iloc[:-1]\n",
    "                return df.assign(home=0, game_id=count)\n",
    "\n",
    "            # combine both team data and export all match data to csv\n",
    "            t1 = get_team_1_player_data()\n",
    "            t2 = get_team_2_player_data()\n",
    "            player_data = pd.concat([player_data, pd.concat([t1,t2]).reset_index()])\n",
    "            \n",
    "            print(f'{count+1}/{len(match_links)} matches collected')\n",
    "            player_data.to_csv(f'Data/RAW/FBREF_Dataset/{season.lower()}/{league.lower()}_{season.lower()}_player_data.csv', \n",
    "                header=True, index=False, mode='w')\n",
    "        except Exception as e:\n",
    "            print(f'{link}: error - {e}')\n",
    "        # sleep for 3 seconds after every game to avoid IP being blocked\n",
    "        time.sleep(3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting fixture data...\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 429: Too Many Requests",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m         main()\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)\n",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(): \n\u001b[1;32m      3\u001b[0m     url, league, season \u001b[38;5;241m=\u001b[39m get_data_info()\n\u001b[0;32m----> 4\u001b[0m     get_fixture_data(url, league, season)\n\u001b[1;32m      5\u001b[0m     match_links \u001b[38;5;241m=\u001b[39m get_match_links(url, league)\n\u001b[1;32m      6\u001b[0m     player_data(match_links, league, season)\n",
      "Cell \u001b[0;32mIn[1], line 70\u001b[0m, in \u001b[0;36mget_fixture_data\u001b[0;34m(url, league, season)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# create empty data frame and access all tables in url\u001b[39;00m\n\u001b[1;32m     69\u001b[0m fixturedata \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([])\n\u001b[0;32m---> 70\u001b[0m tables \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_html(url)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# get fixtures\u001b[39;00m\n\u001b[1;32m     73\u001b[0m fixtures \u001b[38;5;241m=\u001b[39m tables[\u001b[38;5;241m0\u001b[39m][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWk\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDay\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHome\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAway\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxG.1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-HanoiUniversityofScienceandTechnology/Study/School/2024.1/Data Science/Project/Football_player_price_prediction/Scraping/lib/python3.11/site-packages/pandas/io/html.py:1240\u001b[0m, in \u001b[0;36mread_html\u001b[0;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   1225\u001b[0m     [\n\u001b[1;32m   1226\u001b[0m         is_file_like(io),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1230\u001b[0m     ]\n\u001b[1;32m   1231\u001b[0m ):\n\u001b[1;32m   1232\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal html to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_html\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1234\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\n\u001b[1;32m   1241\u001b[0m     flavor\u001b[38;5;241m=\u001b[39mflavor,\n\u001b[1;32m   1242\u001b[0m     io\u001b[38;5;241m=\u001b[39mio,\n\u001b[1;32m   1243\u001b[0m     match\u001b[38;5;241m=\u001b[39mmatch,\n\u001b[1;32m   1244\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   1245\u001b[0m     index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[1;32m   1246\u001b[0m     skiprows\u001b[38;5;241m=\u001b[39mskiprows,\n\u001b[1;32m   1247\u001b[0m     parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[1;32m   1248\u001b[0m     thousands\u001b[38;5;241m=\u001b[39mthousands,\n\u001b[1;32m   1249\u001b[0m     attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1250\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   1251\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   1252\u001b[0m     converters\u001b[38;5;241m=\u001b[39mconverters,\n\u001b[1;32m   1253\u001b[0m     na_values\u001b[38;5;241m=\u001b[39mna_values,\n\u001b[1;32m   1254\u001b[0m     keep_default_na\u001b[38;5;241m=\u001b[39mkeep_default_na,\n\u001b[1;32m   1255\u001b[0m     displayed_only\u001b[38;5;241m=\u001b[39mdisplayed_only,\n\u001b[1;32m   1256\u001b[0m     extract_links\u001b[38;5;241m=\u001b[39mextract_links,\n\u001b[1;32m   1257\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1258\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   1259\u001b[0m )\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-HanoiUniversityofScienceandTechnology/Study/School/2024.1/Data Science/Project/Football_player_price_prediction/Scraping/lib/python3.11/site-packages/pandas/io/html.py:983\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    972\u001b[0m p \u001b[38;5;241m=\u001b[39m parser(\n\u001b[1;32m    973\u001b[0m     io,\n\u001b[1;32m    974\u001b[0m     compiled_match,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    979\u001b[0m     storage_options,\n\u001b[1;32m    980\u001b[0m )\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 983\u001b[0m     tables \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mparse_tables()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseekable\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io\u001b[38;5;241m.\u001b[39mseekable():\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-HanoiUniversityofScienceandTechnology/Study/School/2024.1/Data Science/Project/Football_player_price_prediction/Scraping/lib/python3.11/site-packages/pandas/io/html.py:249\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_tables(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_doc(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs)\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-HanoiUniversityofScienceandTechnology/Study/School/2024.1/Data Science/Project/Football_player_price_prediction/Scraping/lib/python3.11/site-packages/pandas/io/html.py:806\u001b[0m, in \u001b[0;36m_LxmlFrameParser._build_doc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 806\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(r, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_content\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-HanoiUniversityofScienceandTechnology/Study/School/2024.1/Data Science/Project/Football_player_price_prediction/Scraping/lib/python3.11/site-packages/pandas/io/html.py:785\u001b[0m, in \u001b[0;36m_LxmlFrameParser._build_doc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio):\n\u001b[0;32m--> 785\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    786\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options\n\u001b[1;32m    787\u001b[0m         ) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    788\u001b[0m             r \u001b[38;5;241m=\u001b[39m parse(f\u001b[38;5;241m.\u001b[39mhandle, parser\u001b[38;5;241m=\u001b[39mparser)\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# try to parse the input in the simplest way\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-HanoiUniversityofScienceandTechnology/Study/School/2024.1/Data Science/Project/Football_player_price_prediction/Scraping/lib/python3.11/site-packages/pandas/io/common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m _get_filepath_or_buffer(\n\u001b[1;32m    729\u001b[0m     path_or_buf,\n\u001b[1;32m    730\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m    731\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    732\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    733\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    734\u001b[0m )\n\u001b[1;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    737\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-HanoiUniversityofScienceandTechnology/Study/School/2024.1/Data Science/Project/Football_player_price_prediction/Scraping/lib/python3.11/site-packages/pandas/io/common.py:384\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    383\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urlopen(req_info) \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    385\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-HanoiUniversityofScienceandTechnology/Study/School/2024.1/Data Science/Project/Football_player_price_prediction/Scraping/lib/python3.11/site-packages/pandas/io/common.py:289\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-HanoiUniversityofScienceandTechnology/Study/School/2024.1/Data Science/Project/Football_player_price_prediction/Scraping/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-HanoiUniversityofScienceandTechnology/Study/School/2024.1/Data Science/Project/Football_player_price_prediction/Scraping/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-HanoiUniversityofScienceandTechnology/Study/School/2024.1/Data Science/Project/Football_player_price_prediction/Scraping/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-HanoiUniversityofScienceandTechnology/Study/School/2024.1/Data Science/Project/Football_player_price_prediction/Scraping/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-HanoiUniversityofScienceandTechnology/Study/School/2024.1/Data Science/Project/Football_player_price_prediction/Scraping/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-HanoiUniversityofScienceandTechnology/Study/School/2024.1/Data Science/Project/Football_player_price_prediction/Scraping/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 429: Too Many Requests"
     ]
    }
   ],
   "source": [
    "# main function\n",
    "def main(): \n",
    "    url, league, season = get_data_info()\n",
    "    get_fixture_data(url, league, season)\n",
    "    match_links = get_match_links(url, league)\n",
    "    player_data(match_links, league, season)\n",
    "\n",
    "    # checks if user wants to collect more data\n",
    "    print('Data collected!')\n",
    "    while True:\n",
    "        answer = input('Do you want to collect more data? (yes/no): ')\n",
    "        if answer == 'yes':\n",
    "            main()\n",
    "        if answer == 'no':\n",
    "            sys.exit()\n",
    "        else:\n",
    "            print('Answer not valid')\n",
    "            continue\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        main()\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(err.response.status_code)\n",
    "        print(err.response.text)\n",
    "        print('The website refused access, try again later')\n",
    "        time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
